如何找到啤酒和尿布的组合
关联分析Apriori
Association Rules
美国明尼苏达州一家Target被客户投诉，一位中年男子指控Target将婴儿产品优惠券寄给他的女儿
（高中生）。但没多久他却来电道歉，因为女儿经他逼问后坦承自己真的怀孕了。
关联规则
Association Rules
关联规则：Association Rules，或者是
Basket Analysis
解释了：如果一个消费者购买了产品A，那
么他有多大几率会购买产品B?
关联规则
Association Rules
啤酒和尿布：
沃尔玛在分析销售记录时，发现啤酒和尿布经常一起被购买，于是他们调整了货架，把两者放在一起，结果真的
提升了啤酒的销量。
原因解释：爸爸在给宝宝买尿布的时候，会顺便给自己买点啤酒？
沃尔玛是最早通过大数据分析而受益的传统零售企业，对消费者购物行为进行跟踪和分析。
关联规则
Association Rules
支持度、置信度和提升度
支持度：是个百分比，指的是某个商品组合出
现的次数与总次数之间的比例。支持度越高，
代表这个组合出现的频率越大。
“牛奶”的支持度=4/5=0.8
“牛奶+面包”的支持度=3/5=0.6。
订单编号
购买的商品
1
牛奶、面包、尿布
2
可乐、面包、尿布、啤酒
3
牛奶、尿布、啤酒、鸡蛋
4
面包、牛奶、尿布、啤酒
5
面包、牛奶、尿布、可乐
Association Rules
支持度、置信度和提升度
置信度：是个条件概念
指的是当你购买了商品A，会有多大的概率购
买商品B
置信度（牛奶→啤酒）=2/4=0.5
置信度（啤酒→牛奶）=2/3=0.67
订单编号
购买的商品
1
牛奶、面包、尿布
2
可乐、面包、尿布、啤酒
3
牛奶、尿布、啤酒、鸡蛋
4
面包、牛奶、尿布、啤酒
5
面包、牛奶、尿布、可乐
Association Rules
支持度、置信度和提升度
提升度：商品A的出现，对商品B的出现概率提
升的程度
如果我们单纯看置信度(可乐→尿布)=1，也就
是说可乐出现的时候，用户都会购买尿布，那
么当用户购买可乐的时候，就需要推荐尿布么？
订单编号
购买的商品
1
牛奶、面包、尿布
2
可乐、面包、尿布、啤酒
3
牛奶、尿布、啤酒、鸡蛋
4
面包、牛奶、尿布、啤酒
5
面包、牛奶、尿布、可乐
Association Rules
支持度、置信度和提升度
提升度：商品A的出现，对商品B的出现概率提升的程度
提升度(A→B)=置信度(A→B)/支持度(B)
提升度的三种可能：
• 提升度(A→B)>1：代表有提升；
• 提升度(A→B)=1：代表有没有提升，也没有下降；
• 提升度(A→B)<1：代表有下降。
订单编号
购买的商品
1
牛奶、面包、尿布
2
可乐、面包、尿布、啤酒
3
牛奶、尿布、啤酒、鸡蛋
4
面包、牛奶、尿布、啤酒
5
面包、牛奶、尿布、可乐
Association Rules
Apriori算法原理
我们把上面案例中的商品用ID来代表，牛奶、面包、尿
布、可乐、啤酒、鸡蛋的商品ID分别设置为1-6
Apriori算法就是查找频繁项集(frequent itemset)的过程
频繁项集：支持度大于等于最小支持度(Min Support)阈
值的项集。
非频繁项集：支持度小于最小支持度的项集
订单编号
购买的商品
1
1、2、3
2
4、2、3、5
3
1、3、5、6
4
2、1、3、5
5
2、1、3、4
Association Rules
Apriori算法原理
先计算K=1项的支持度
商品项集
支持度
1
4/5
2
4/5
3
5/5
4
2/5
5
3/5
6
1/5
假设最小支持度=0.5，那么Item4和6不
符合最小支持度的，不属于频繁项集
商品项集
支持度
1
4/5
2
4/5
3
5/5
5
3/5
Association Rules
Apriori算法原理
在这个基础上，我们将商品两两组合，得
到k=2项的支持度
筛选掉小于最小值支持度的商品组合
商品项集
支持度
1，2
3/5
1，3
4/5
1，5
2/5
2，3
4/5
2，5
2/5
3，5
3/5
商品项集
支持度
1，2
3/5
1，3
4/5
2，3
4/5
3，5
3/5
Association Rules
Apriori算法原理
将商品进行K=3项的商品组合，可以得到：筛选掉小于最小值支持度的商品组合
商品项集
支持度
1，2，3
3/5
2，3，5
2/5
1，2，5
1/5
商品项集
支持度
1，2，3
3/5
得到K=3项的频繁项集{1,2,3}，也就是{
牛奶、面包、尿布}的组合。
Association Rules
Apriori算法原理
Apriori算法的流程：
Step1，K=1，计算K项集的支持度；
Step2，筛选掉小于最小支持度的项集；
Step3，如果项集为空，则对应K-1项集的结果为最终结果。
否则K=K+1，重复1-3步。
How to use Apriori algorithm for association analysis
如何使用Apriori算法做关联分析
使用工具包：
from efficient_apriori import apriori
或者：
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
How to use Apriori algorithm for association analysis
如何使用Apriori算法做关联分析
from efficient_apriori import apriori
# 设置数据集
transactions = [('牛奶','面包','尿布'),
('可乐','面包', '尿布', '啤酒'),
('牛奶','尿布', '啤酒', '鸡蛋'),
('面包', '牛奶', '尿布', '啤酒'),
('面包', '牛奶', '尿布', '可乐')]
# 挖掘频繁项集和频繁规则
itemsets, rules = apriori(transactions, min_support=0.5,  min_confidence=1)
print("频繁项集：", itemsets)
print("关联规则：", rules)
订单编号
购买的商品
1
牛奶、面包、尿布
2
可乐、面包、尿布、啤酒
3
牛奶、尿布、啤酒、鸡蛋
4
面包、牛奶、尿布、啤酒
5
面包、牛奶、尿布、可乐
How to use Apriori algorithm for association analysis
如何使用Apriori算法做关联分析
频繁项集：{1: {('啤酒',): 3, ('尿布',): 5, ('牛奶',): 4, ('面包',): 
4}, 2: {('啤酒', '尿布'): 3, ('尿布', '牛奶'): 4, ('尿布', '面包'): 4, 
('牛奶', '面包'): 3}, 3: {('尿布', '牛奶', '面包'): 3}}
关联规则：[{啤酒} -> {尿布}, {牛奶} -> {尿布}, {面包} -> 
{尿布}, {牛奶, 面包} -> {尿布}]
订单编号
购买的商品
1
牛奶、面包、尿布
2
可乐、面包、尿布、啤酒
3
牛奶、尿布、啤酒、鸡蛋
4
面包、牛奶、尿布、啤酒
5
面包、牛奶、尿布、可乐
Use cases for association analysis
关联分析的使用场景
万物皆Transaction：
• 超市购物小票（TransactionID => Item）
• 每部电影的分类（MovieID => 分类）
• 每部电影的演员（MovieID => 演员）
Use cases for association analysis
关联分析的使用场景
超市购物小票的关联关系
每笔订单的商品（TransactionID => Item）
How to use Apriori algorithm for association analysis
如何使用Apriori算法做关联分析
BreadBasket数据集（21293笔订单）：
BreadBasket_DMS.csv
字段：Date（日期），Time（时间），Transaction（交易ID）Item（商品名称）
交易ID的范围是[1,9684]，存在交易ID为空的情况，同一笔交易中存在商品重
复的情况。以外，有些交易没有购买商品（对应的Item为NONE）
How to use Apriori algorithm for association analysis
如何使用Apriori算法做关联分析
# 数据加载
data = pd.read_csv('./BreadBasket_DMS.csv')
# 统一小写
data['Item'] = data['Item'].str.lower()
# 去掉none项
data = data.drop(data[data.Item == 'none'].index)
How to use Apriori algorithm for association analysis
如何使用Apriori算法做关联分析
# 得到一维数组orders_series，并且将Transaction作为
index, value为Item取值
orders_series = data.set_index('Transaction')['Item']
# 将数据集进行格式转换
transactions = []
temp_index = 0
for i, v in orders_series.items():
if i != temp_index:
temp_set = set()
temp_index = i
temp_set.add(v)
transactions.append(temp_set)
else:
temp_set.add(v)
[{'scandinavian'}, {'hot chocolate', 'jam', 'cookies'}, 
{'muffin'}, {'bread', 'coffee', 'pastry'}, {'medialuna', 
'pastry', 'muffin'}, {'tea', 'coffee', 'medialuna', 'pastry'}, 
{'bread', 'pastry'}, {'bread', 'muffin'}, {'scandinavian', 
'medialuna'}……
transaction：订单数组
每笔订单为一个集合，去掉订单中的重复项
How to use Apriori algorithm for association analysis
如何使用Apriori算法做关联分析
itemsets, rules = apriori(transactions, min_support=0.02,  
min_confidence=0.5)
通过调整min_support，min_confidence可以得到不同的频
繁项集和关联规则
min_support=0.02，min_confidence=0.5时
一共有33个频繁项集，8种关联规则
频繁项集：{1: {('alfajores',): 344, ('bread',): 3096, 
('brownie',): 379, ('cake',): 983, ('coffee',): 4528, ('cookies',): 
515, ('farm house',): 371, ('hot chocolate',): 552, ('juice',): 
365, ('medialuna',): 585, ('muffin',): 364, ('pastry',): 815, 
('sandwich',): 680, ('scandinavian',): 275, ('scone',): 327, 
('soup',): 326, ('tea',): 1350, ('toast',): 318, ('truffles',): 192}, 2: 
{('bread', 'cake'): 221, ('bread', 'coffee'): 852, ('bread', 
'pastry'): 276, ('bread', 'tea'): 266, ('cake', 'coffee'): 518, 
('cake', 'tea'): 225, ('coffee', 'cookies'): 267, ('coffee', 'hot 
chocolate'): 280, ('coffee', 'juice'): 195, ('coffee', 'medialuna'): 
333, ('coffee', 'pastry'): 450, ('coffee', 'sandwich'): 362, 
('coffee', 'tea'): 472, ('coffee', 'toast'): 224}}
关联规则：[{cake} -> {coffee}, {cookies} -> {coffee}, {hot 
chocolate} -> {coffee}, {juice} -> {coffee}, {medialuna} -> 
{coffee}, {pastry} -> {coffee}, {sandwich} -> {coffee}, {toast} -
> {coffee}]
使用efficient_apriori工具包
效率较高，但返回参数较少
How to use Apriori algorithm for association analysis
如何使用Apriori算法做关联分析
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
hot_encoded_df=data.groupby(['Transaction','Item'])['Item'].count().unstack().r
eset_index().fillna(0).set_index('Transaction')
hot_encoded_df = hot_encoded_df.applymap(encode_units)
frequent_itemsets = apriori(hot_encoded_df, min_support=0.02, 
use_colnames=True)
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=0.5)
使用mlxtend.frequent_patterns工具包
效率较低，但返回参数较多
Use cases for association analysis
关联分析的使用场景
电影分类中的关联关系
每部电影的分类（MovieID => 分类）
Case: Association Analysis in Movie Classification
案例：电影分类中的关联分析
数据集：MovieLens
下载地址：https://www.kaggle.com/jneupane12/movielens/download
主要使用的文件：movies.csv
格式：movieId
title
genres
记录了电影ID，标题和分类
我们可以分析下电影分类之间的频繁项集和关联规则
MovieLens 主要使用Collaborative Filtering 和Association Rules 相结合的技术，向用户推荐他们感兴
趣的电影。
Case: Association Analysis in Movie Classification
案例：电影分类中的关联分析
# 将genres进行one-hot编码（离散特征有多少取值，就用多少维来表示这个
特征）
movies_hot_encoded = 
movies.drop('genres',1).join(movies.genres.str.get_dummies())
# 将movieId, title设置为index
movies_hot_encoded.set_index(['movieId','title'],inplace=True)
# 挖掘频繁项集，最小支持度为0.02
itemsets = apriori(movies_hot_encoded,use_colnames=True, min_support=0.02)
# 根据频繁项集计算关联规则，设置最小提升度为2
rules =  association_rules(itemsets, metric='lift', min_threshold=2)
Case: Association Analysis in Movie Classification
案例：电影分类中的关联分析
antecedents
consequents
antecedent 
support
consequent 
support
support
confidence
lift
leverage
conviction
9
frozenset({'Mystery'})
frozenset({'Thriller'})
0.055502603
0.153163722
0.029144365
0.525099075
3.428351502
0.02064338 1.78318515
4
8
frozenset({'Thriller'})
frozenset({'Mystery'})
0.153163722
0.055502603
0.029144365
0.190282432
3.428351502
0.02064338 1.16645289
15
frozenset({'Crime'})
frozenset({'Drama', 
'Thriller'})
0.107742503
0.068480094
0.024965173
0.231711466
3.383632432
0.01758695
7
1.21246102
9
12
frozenset({'Drama', 'Thriller'})
frozenset({'Crime'})
0.068480094
0.107742503
0.024965173
0.364561028
3.383632432
0.01758695
7
1.40415922
8
7
frozenset({'Action'})
frozenset({'Adventure'})
0.129041719
0.08538016
0.035633111
0.276136364
3.234198251
0.02461550
8
1.26352505
4
6
frozenset({'Adventure'})
frozenset({'Action'})
0.08538016
0.129041719
0.035633111
0.417346501
3.234198251
0.02461550
8
1.49481343
9
16
frozenset({'Action'})
frozenset({'Sci-Fi'})
0.129041719
0.063897646
0.02349879
0.182102273
2.849905792
0.01525332
8
1.14452250
2
17
frozenset({'Sci-Fi'})
frozenset({'Action'})
0.063897646
0.129041719
0.02349879
0.367756741
2.849905792
0.01525332
8
1.37756831
6
0
frozenset({'Thriller'})
frozenset({'Crime'})
0.153163722
0.107742503
0.045164602
0.294877932
2.736876567
0.02866235
9
1.26539437
3
1
frozenset({'Crime'})
frozenset({'Thriller'})
0.107742503
0.153163722
0.045164602
0.419190201
2.736876567
0.02866235
9
1.45802684
4
5
frozenset({'Horror'})
frozenset({'Thriller'})
0.095718161
0.153163722
0.039335728
0.410953658
2.683100496
0.02467517
9
1.43763948
2
4
frozenset({'Thriller'})
frozenset({'Horror'})
0.153163722
0.095718161
0.039335728
0.256821446
2.683100496
0.02467517
9
1.21677601
4
Use cases for association analysis
关联分析的使用场景
电影演员中的关联关系
每部电影的演员列表（MovieID => 演员）
Case: Association Analysis among Movie Actors
案例：电影演员中的关联分析
数据集：MovieActors
来源：movie_actors.csv
爬虫抓取movie_actors_download.py
格式：title
actors
记录了电影标题和演员列表
我们可以分析下电影演员之间的频繁
项集和关联规则
Case: Association Analysis among Movie Actors
案例：电影演员中的关联分析
from selenium import webdriver
# 设置想要下载的导演 数据集
director = u'徐峥'
base_url = 'https://movie.douban.com/subject_search?search_text='+director+'&cat=1002&start='
# 下载指定页面的数据
def download(request_url):
# 将字典类型转化为DataFrame
movie_actors = pd.DataFrame(movie_actors, index=[0])
# DataFrame 行列转换
movie_actors = pd.DataFrame(movie_actors.values.T, index=movie_actors.columns, 
columns=movie_actors.index)
movie_actors.index.name = 'title'
movie_actors.set_axis(['actors'], axis='columns', inplace=True)
movie_actors.to_csv('./movie_actors.csv')
• 当没有现成的数据源时，可以通过
爬虫进行数据抓取，保存在csv文
件
• 爬虫抓取属于提升部分，不是本次
课重点
• 重点理解：万物皆Transaction
• 掌握方法：挖掘数据集中的频繁项
集和关联规则
Case: Association Analysis among Movie Actors
案例：电影演员中的关联分析
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
# 数据加载
movies = pd.read_csv('./movie_actors.csv')
# 将genres进行one-hot编码（离散特征有多少取值，就用多少维来表示这个特征）
movies_hot_encoded = movies.drop('actors',1).join(movies.actors.str.get_dummies('/'))
# 将movieId, title设置为index
movies_hot_encoded.set_index(['title'],inplace=True)
# 挖掘频繁项集，最小支持度为0.05
itemsets = apriori(movies_hot_encoded,use_colnames=True, min_support=0.05)
# 按照支持度从大到小进行时候粗
itemsets = itemsets.sort_values(by="support" , ascending=False) 
Case: Association Analysis among Movie Actors
案例：电影演员中的关联分析
pd.options.display.max_columns=100
# 根据频繁项集计算关联规则，设置最小提升度为2
rules =  association_rules(itemsets, metric='lift', 
min_threshold=2)
# 按照提升度从大到小进行排序
rules = rules.sort_values(by="lift" , ascending=False) 
#rules.to_csv('./rules.csv')
Association rules perspective
关联规则的视角
不需要考虑用户一定时期内的偏好，而是基于Transaction
只要能将数据转换成Transaction，就可以做购物篮分析：
Step1、把数据整理成id=>item形式，转换成transaction
Step2、设定关联规则的参数（support、confident）挖掘关联规则
Step3、按某个指标（lift、support等）对以关联规则排序
How to determine the minimum support and minimum confidence in association rules
关联规则中的最小支持度、最小置信度该如何确定
• 最小支持度，最小置信度是实验出来的
• 最小支持度：
不同的数据集，最小值支持度差别较大。可能是0.01到0.5之间
可以从高到低输出前20个项集的支持度作为参考
最小置信度：可能是0.5到1之间
提升度：表示使用关联规则可以提升的倍数，是置信度与期望置信度的比值
提升度至少要大于1
Summary
总结
• 支持度、置信度、提升度
• 最小支持度，最小置信度是实验出来的
• 基于关联规则的推荐算法：
Apriori算法
FPGrowth算法
PrefixSpan算法
• 万物皆Transaction：
超市购物小票（TransactionID => Item）
每部电影的分类（MovieID => 分类）
每部电影的演员（MovieID => 演员）
Thank You
Using data to solve problems
